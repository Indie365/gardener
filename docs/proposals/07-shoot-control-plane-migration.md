# Shoot Control Plane Migration

## Motivation

Currently moving the control plane of a shoot cluster can only be done manually and requires deep knowledge of how exactly to transfer the resources and state from one seed to another. This can make it slow and prone to errors.

Automatic migration can be very useful in a couple of scenarios:
- Seed goes down and can't be repaired (fast enough or at all) and it's control planes need to be brought to another seed
- Seed needs to be changed, but this operation requires the recreation of the seed (e.g. turn a single-AZ seed into a multi-AZ seed)
- Seeds need to be rebalanced
- New seeds become available in a region closer to/in the region of the workers and the control plane should be moved there to improve latency
- Gardener ring, which is a self-supporting setup/underlay for a highly available (usually cross-region) Gardener deployment


## Goals

* Provide a mechanism to migrate the control plane of a shoot cluster from one seed to another
* The mechanism should support migration from a seed which is no longer reachable (Disaster Recovery)
* The shoot cluster nodes are preserved and continue to run the workload, but will talk to the new control plane after the migration completes
* Extension controllers implement a mechanism which allows them to store their state or to be restored from an already existing state on a different seed cluster.
* The already existing shoot reconciliation flow is reused for migration with minimum changes


## Terminology

__Source Seed__ is the seed which currently hosts the control plane of a Shoot Cluster

__Destination Seed__ is the seed to which the control plane is being migrated


## Resources and controller state which have to be migrated between two seeds:

**Note:** The following lists are just FYI and are meant to show the current resources which need to be moved to the __Destination Seed__

### Secrets

Gardener has preconfigured lists of needed secrets which are generated when a shoot is created and deployed in the seed. Following is a minimum set of secrets which must be migrated to the __Destination Seed__. Other secrets can be regenerated from them.

- ca
- ca-front-proxy
- static-token
- ca-kubelet
- ca-metrics-server
- etcd-encryption-secret
- kube-aggregator
- kube-apiserver-basic-auth
- kube-apiserver
- service-account-key
- ssh-keypair

### Custom Resources and state of extension controllers

Gardenlet deploys custom resources in the __Source Seed__ cluster during shoot reconciliation which are reconciled by extension controllers. The state of these controllers and any additional resources they create is independent of the gardenlet and must also be migrated to the __Destination Seed__. Following is a list of custom resources, and the state which is generated by them that has to be migrated.

- **BackupBucket**: nothing relevant for migration
- **BackupEntry**: nothing relevant for migration
- **ControlPlane**: nothing relevant for migration
- **DNSProvider**/DNSEntry: nothing relevant for migration
- **Extensions**: migration of state needs to be handled individually
- **Infrastructure**: terraform state
- **Network**: nothing relevant for migration
- **OperatingSystemConfig**: nothing relevant for migration
- **Worker**: Machine-Controller-Manager related objects: machineclasses, machinedeployments, machinesets, machines

This list depends on the currently installed extensions and can change in the future

## Proposal

### Custom Resource on the garden cluster

The Garden cluster has a new Custom Resource which is stored in the project namespace of the Shoot called `ShootState`. It contains all the required data described above so that the control plane can be recreated on the __Destination Seed__.

This data is separated into two sections. The first is generated by the gardenlet and then either used to generate new resources (e.g secrets) or is directly deployed to the Shoot's control plane on the __Destination Seed__.

The second is generated by the extension controllers in the seed.

```yaml
apiVersion: core.gardener.cloud/v1alpha1
kind: ShootState
metadata:
  name: my-shoot
  namespace: garden-core
  ownerReference:
    apiVersion: core.gardener.cloud/v1beta1
    blockOwnerDeletion: true
    controller: true
    kind: Shoot
    name: my-shoot
    uid: ...
  finalizers:
  - gardener
gardenlet:
  secrets:
  - name: ca
    data:
      ca.crt: ...
      ca.key: ...
  - name: ssh-keypair
    data:
      id_rsa: ...
  - name:
...
extensions:
- kind: Infrastructure
  state: ... (Terraform state)
- kind: ControlPlane
  purpose: normal
  state: ... (Certificates generated by the extension)
- kind: Worker
  state: ... (Machine objects)
```

The state data is saved as a `runtime.RawExtension` type, which can be encoded/decoded by the corresponding extension controller.

There can be sensitive data in the `ShootState` which has to be hidden from the end-users. Hence, it will be recommended to provide an etcd encryption configuration to the Gardener API server in order to encrypt the `ShootState` resource.

#### Size limitations

There are limits on the size of the request bodies sent to the kubernetes API server when creating or updating resources: by default ETCD can only accept request bodies which do not exceed 1.5 MiB (this can be configured with the `--max-request-bytes` flag); the kubernetes API Server has a request body limit of 3 MiB which cannot be set from the outside (with a command line flag); the gRPC configuration used by the API server to talk to ETCD has a limit of 2 MiB per request body which cannot be configured from the outside; and `watch` requests have a 16 MiB limit on the buffer used to stream resources.

This means that if `ShootState` is bigger than 1.5 MiB, the ETCD max request bytes will have to be increased. However, there is still an upper limit of 2 MiB imposed by the gRPC configuration.

If `ShootState` exceeds this size limitation it must make use of configmap/secret references to store the state of extension controllers. This is an implementation detail of Gardener and can be done at a later time if necessary as extensions will not be affected.

Splitting the `ShootState` into multiple resources could have a positive benefit on performance as the Gardener API Server and Gardener Controller Manager would handle multiple small resources instead of one big resource.

### Gardener extensions changes

All extension controllers which require state migration must save their state in a new `status.state` field and act on an annotation `gardener.cloud/operation=restore` in the respective Custom Resources which should trigger a restoration operation instead of reconciliation. A restoration operation means that the extension has to restore its state in the Shoot's namespace on the __Destination Seed__ from the `status.state` field.

As an example: the `Infrastructure` resource must save the terraform state.

```
apiVersion: extensions.gardener.cloud/v1alpha1
kind: Infrastructure
metadata:
  name: infrastructure
  namespace: shoot--foo--bar
spec:
  type: azure
  region: eu-west-1
  secretRef:
    name: cloudprovider
    namespace: shoot--foo--bar
  providerConfig:
    apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
    kind: InfrastructureConfig
    resourceGroup:
      name: mygroup
    networks:
      vnet: # specify either 'name' or 'cidr'
      # name: my-vnet
        cidr: 10.250.0.0/16
      workers: 10.250.0.0/19
status:
  state: |
      {
          "version": 3,
          "terraform_version": "0.11.14",
          "serial": 2,
          "lineage": "3a1e2faa-e7b6-f5f0-5043-368dd8ea6c10",
          "modules": [
              {
              }
          ]
          ...
      }
```

Extensions which do not require state migration should set `status.state=nil` in their Custom Resources and trigger a normal reconciliation operation if the CR contains the `core.gardener.cloud/operation=restore` annotation.

Similar to the contract for the [reconcile operation](https://github.com/gardener/gardener/blob/master/docs/extensions/reconcile-trigger.md), the extension controller has to remove the `restore` annotation after the restoration operation has finished.

An additional annotation `gardener.cloud/operation=migrate` is added to the Custom Resources. It is used to tell the extension controllers in the __Source Seed__ that they must stop reconciling resources (in case they are requeued due to errors) and should perform cleanup activities in the Shoot's control plane. These cleanup activities involve removing the finalizers on Custom Resources and deleting them without actually deleting any infrastructure resources.

**Note:** The same size limitations from the previous section are relevant here as well.

### Shoot reconciliation flow changes

The only data which must be stored in the `ShootState` by the gardenlet is secrets (e.g ca for the API server). Therefore the `botanist.DeploySecrets` step is changed. It is split into two functions which take a list of secrets that have to be generated.
- `botanist.GenerateSecretState` Generates certificate authorities and other secrets which have to be persisted in the ShootState and must not be regenerated on the __Destination Seed__.
- `botanist.DeploySecrets` Takes secret data from the `ShootState`, generates new ones (e.g. client tls certificates from the saved certificate authorities) and deploys everything in the Shoot's control plane on the __Destination Seed__


### ShootState synchronization controller

The ShootState synchronization controller will become part of the gardenlet. It syncs the state of extension custom resources from the shoot namespace to the garden cluster and updates the corresponding `spec.extension.state` field in the `ShootState` resource. The controller can `watch` Custom Resources used by the extensions and update the `ShootState` only when changes occur.


### Migration workflow
1. Starting migration
    - Migration can only be started after a Shoot cluster has been successfully created so that the `status.seed` field in the `Shoot` resource has been set
    - The `Shoot` resource's field `spec.seedName="new-seed"` is edited to hold the name of the __Destination Seed__ and reconciliation is automatically triggered
    - The Garden Controller Manager checks if the equality between `spec.seedName` and `status.seed`, detects that they are different and triggers migration.
1. The Garden Controller Manager waits for the __Destination Seed__ to be ready
1. Shoot's API server is stopped
1. Backup the Shoot's ETCD.
1. Extension resources in the __Source Seed__ are annotated with `gardener.cloud/operation=migrate`
1. Scale Down the Shoot's control plane in the __Source Seed__.
1. The gardenlet in the __Destination Seed__ fetches the state of extension resources from the `ShootState` resource in the garden cluster.
1. Normal reconciliation flow is resumed in the __Destination Seed__. Extension resources are annotated with `gardener.cloud/operation=restore` to instruct the extension controllers to reconstruct their state.
1. The Shoot's namespace in __Source Seed__ is deleted.


### Migrating Backups

For security reasons a `Seed` should only have access to its own backup bucket, meaning that the `etcd-backup-restore` sidecar responsible for saving and restoring the ETCD data should only be able to access the backup bucket of the `Seed` it currently operates in. Therefore, during Control Plane Migration a mechanism is needed to copy the ETCD backup from the __Source Seed__'s backup bucket to that of the __Destination Seed__. To that end `etcd-backup-restore` is extended with a new subcommand - `migrate` which runs in the Seed and does the following:

1. Creates a backup directory for the `Shoot` cluster in the __Destination Seed__'s backup bucket if it does not already exist.
1. Waits for some time (until a preconfigured timeout) for the `etcd-backup-restore` sidecar in the __Source Seed__ to finish saving a final snapshot.
1. Writes into a "header" file inside the `Shoot`'s backup directory in the __Source Seed__'s backup bucket that copying of data __from__ this directory is in process, the timestamp when the copy operation began, a timestamp indicating when migration was actually triggered and the name of the __Destination Seed__
1. Writes into a "header" file inside the `Shoot`'s backup directory in the __Destination Seed__'s backup bucket that copying of data __to__ this directory is in process, the timestamp of when the copy operation began and a timestamp indicating when migration was actually triggered.
1. When the `etcd-backup-restore` sidecar in the __Destination Seed__ tries to restore the data it checks the "header":
  - If data is still being copied, it does nothing and dies.
  - If the data has finished copying, it restores the ETCD from it.
1. When the copy operation has finished successfully, the "header" files in both the source and destination backup directories are updated accordingly.
1. Subsequent executions of the command (in case the operation is repeated as part of the Control Plane Migration flow) check that the data has already been copied and do nothing

*Note:* the migration timestamp might be redundant as it will closely correspond to the timestamp indicating the start of the copy operation.
*Note:* Any additional checks and steps that have to be taken by the `etcd-backup-restore` sidecar in the __Source Seed__ in case there are problems with the Seed are described in more detail in the __Leader Election__ section below.

The `migrate` subcommand requires secrets for both the backup buckets of the __Source__ and __Destination__ Seeds. The latter is already taken care of by the Backup Entry controller. For the former - a temporary secret needs to be created and provided to the Seed cluster. The exact mechanism of how this will be done is to be discussed in more details in [Dedicated secrets per seed backup bucket](https://github.com/gardener-security/security-backlog/issues/17) and [Eliminate all trust credentials between hyperscalers and Gardener](https://github.com/gardener-security/security-backlog/issues/25)

The ETCD Druid is extended so that it starts `etcd-backup-restore migrate` as a job when it is notified that Control Plane Migration has been triggered. This notification happens via a `gardener.cloud/operation=restore` annotation in the `etcd` resource and (if necessary) a few additional fields indicating the cluster owner and when migration was triggered:
```
spec:
  ownerToken: <seed_name>
  ownerTimestamp: "2018-11-18T15:14:57Z"
```
The ETCD Druid will automatically reconcile the `etcd` resource as normal after it has finished with the `etcd-backup-restore migrate` operation.
### Leader Election

**Note**: This section is still under discussion. The plan is to first implement the `ShootState` and modify the reconciliation flow and extensions accordingly. Additionally multiple scenarios need to be considered depending on the reachability to the Garden cluster from the __Source Seed__ components, the __Source Seed's__ API server from the Garden cluster and the __Source Seed's__ API server from the controllers running on the seed. **The initial implementation will only cover the case where everything is running**.

During migration "split brain" scenario must be avoided. This means that a Shoot's control plane in the __Source Seed__ must be scaled down before it is scaled up in the __Destination Seed__.

### Garden cluster and __Source Seed__ are healthy and there are no network problems

If both the Garden cluster and __Source Seed__ cluster are healthy, the `gardenlet` after checking the `spec.seedName`, can directly scale down the Shoot's control plane as part of the migration flow.

### If components in the __Source Seed__ cannot reliably read who the leader is from the Garden cluster
We consider the following cases in which the `gardenlet` and the extension controllers in the __Source Seed__ cannot fetch information about who the leader is:

To avoid possible reconciliations by Control Plane Components and Extension Controllers in both the __Destination Seed__ and __Source Seed__ we use the following leader election procedure (split into two parts - one for components in the `Shoot`'s control plane which depend on its APIServer and one for extension controllers which do not depend on it):

#### ETCD Backup Based Leader Election

When migration is triggered:
1. The ETCD Druid in the __Destination Seed__ is informed about the ownership of the ETCD cluster and that Control Plane Migration has been triggered
2. It triggeres `etcd-backup-restore migrate` which first checks to see whether a finished snapshot has been made by the `etcd-backup-restore` sidecar in the __Source Seed__ and then begins the copy operation. If no snapshot is created within a timeout it will copy the latest snapshot.
4. The `etcd-backup-restore` sidecar initially (during startup) and continuously (when uploading backups) checks to see if a copy operation to a new bucket has started or not.
    1. If it finds that a copy operation __from__ its bucket is in process or has been finished, it determines that it is in the __Source Seed__ and that control plane migration has been triggered.
    2. If it finds that a copy operation __to__ its bucket is in process, it determines that it is in the __Destination Seed__ and that control plane migration has been triggered. Once it finds that the copy operation has completed successfully it restores the ETCD data.
    3. If the ownership information matches and thare is no indication that a copy operation is in process, `etcd-backup-restore` continues to work exactly the way it does today.
5. When the `etcd-backup-restore` sidecar in the `Shoot's` ETCD pod determines that migration is on and that it is on the __Source Seed__, it stops taking snapshots (full and delta snapshots) and terminates with an error
    1. This should also cut-off the client requests because the `readinessProbe` of the main ETCD container probes the `etcd-backup-restore` sidecar.
    1. Just to be sure, the health check in the backup-restore sidecar (used by the main ETCD container's `readinessProbe`) is enhanced to also compare the ownership information passed by `etcd-druid` with what is in the backup. This should ensure that if the main etcd container `readinessProbe` will continue to fail if the `etcd-backup-restore` sidecar comes back up.
    1. Please note that the etcd container itself might not die here. But the client requests to the container will be cut off.
5. When the `etcd-backup-restore` sidecar in the `Shoot's` ETCD pod determines that migration is on and that it is on the __Destination Seed__, it first waits for the migration process to finish
    1. It then checks the available backups and restores the ETCD data from the snapshot with the latest timestamp (older than the migration timestamp).
7. Once the ETCD container stops accepting the client requests in the __Source Seed__, all components which depend on it (e.g. the Kube APIServer, Machine Controller Manager and so on) will also stop working.


#### Stopping extension controllers
An additional step needs to be taken to prevent the extension controllers from reconciling their resources if they were stuck in a crashloop backoff triggered by a previous reconciliation, which can happen in the following casess:

1. `Gardenlet` cannot connect to the Garden APIServer

    In this case the  `gardenlet` cannot fetch information about `Shoots` and therefore does not know if control plane migration has been triggered. The `gardenlet` will not trigger new reconciliations, however extension controllers can still be reconciling their resources if they are stuck in an error backoff from a previous reconciliation.

2. `Gardenlet` cannot connect to the `Seed`'s APIServer.

    In this case the `gardenlet` knows if migration has been triggered but it will not start shoot migration/reconciliation as it will first check the `Seed` conditions and try to update the `Cluster` resource both of will fail. Extension controllers can still have a connection to the `Seed`'s APIServer (if they are not running where the `gardenlet` is running - different node or cluster) which means that due to previously triggered reconciliations (and/or crashloop backoff) they could reconcile their resources.


3. The `Seed` components (etcd-druid, extension controllers, etc) cannot connect to the `Seed`'s APIServer.

    In this case the extension controllers will not be able to reconcile their resources as they cannot fetch them from the `Seed`'s APIServer. When the connection to the APIServer comes back, the controllers might be stuck in a backoff from a previous reconciliation or the resources could still be annotated with `gardener.cloud/operation=reconcile`. This could lead to a race condition depending on who has a chance to `update` or `get` the resources first - `gardenlet` or the `Seed`'s APIServer. If `gardenlet` manages to update the resource before they are read by the extension controllers, they will be properly updated with `gardener.cloud/operation=migrate`. Otherwise, they would be reconciled as normal.

4. All of the above.

A new `ClusterOwner` resource is introduced in the `Seed` cluster (this does not have to be a new resource - it could just be a `ConfigMap`). It is managed by the `gardenlet` and contains the following:
```
spec:
  owner: <seed-name>
  renewTimestamp: "2020-10-29T15:14:57Z"
  leaseDurationSeconds: 120
```
Every `X` seconds `gardenlet` checks to see if it can connect to the Garden APIServer and if successfull updates the `ClusterOwner` resource and renews the timestamp. The extension controllers fetch this resource before each reconciliation and check if the lease has expired. If it hasn't, then migration has not been triggered and `gardenlet` can reliably tell who the leader is, therefore controllers reconcile as normal. If the lease has expired, then either control plane migration has been triggered or `gardenlet` cannot tell who the leader is.
*Note:* this resource could aslo be managed by the ETCD Druid and updated depending on the health of the `etcd-backup-restore` sidecar as a more reliable source of when the migration was actually triggered as it fetches this information from a "header" file in the `Shoot`'s backup directory. This would require the `etcd` resource to be deployed as soon as possible in the __Destination Seed__ (before other extension resources) so that the `etcd-backup-restore migrate` operation can be triggered to update the "header" in the `Shoot`'s source backup directory which will cause the `etcd-backup-restore` sidecar in the __Source Seed__ to stop and the `ClusterOwner` resource to be updated before extension controllers in the __Destination Seed__ can start reconciling their resources.

**Alternative: DNS leader election:**

A DNS TXT entry with TTL=60s and value seed='__Source Seed__' is used. The record is created and maintained by the Gardener Controller Manager (by using the DNS Controller Manager and its DNSEntry resource). When a control plane migration is detected, the Gardener Controller Manager changes the value of the DNS Entry to seed='__Destination Seed__' and waits for 2*TTL + 1 = 121 seconds to ensure that the change is propagated to all controllers in the old seed. We rely on the fact that DNS is highly available (100% for AWS Route53) and that control plane components in the __Source Seed__ can see the changes.

Control plane components have to be shut down even when there is no access to the __Source Seed's__ API server. To be able to do that a daemonset is deployed in each Seed cluster. When the daemonset in the __Source Seed__ sees that it is nolonger the leader (by checking the DNS record) and there is no connection to the __Source Seed's__ API server, the daemonset will kill the Shoot's control plane pods by directly talking to the Kubelet's API server. If the __Source Seed's__ API Server comes back up, then gardenlet should take care of scaling down the deployments and statefulsets in the Shoot's control plane. This could be problematic if the gardenlet is in a crashloop backoff or takes too much time to do the scaling.

As an alternative to the daemonset, a sidecar container can be added to each control plane component. The sidecar checks the DNS Entry to see if it is still the leader. If it is not, it shuts down the entire pod. This way we do not run the risk of deploymnets and statefulsets recreating the control plane pods after the seed's apiserver comes back up.

The problems with using DNS as leader election is caching. Additionally, not all DNS servers respect TTL settings.